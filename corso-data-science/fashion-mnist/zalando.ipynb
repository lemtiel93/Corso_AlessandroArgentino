{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizza per il singolo canale (grayscale)\n",
    "])\n",
    "# Scaricare e caricare il dataset di training\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Scaricare e caricare il dataset di test\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Creare DataLoader per iterare attraverso il dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # 1 solo canale per scala di grigi\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):  # Ciclo per le epoche\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Azzera i gradienti\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Passaggio forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Passaggio backward e ottimizzazione\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # Stampa ogni 100 batch\n",
    "                print(f'Epoca {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Addestramento completato.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 1, Batch 100, Loss: 0.785\n",
      "Epoca 1, Batch 200, Loss: 0.478\n",
      "Epoca 1, Batch 300, Loss: 0.404\n",
      "Epoca 1, Batch 400, Loss: 0.395\n",
      "Epoca 1, Batch 500, Loss: 0.342\n",
      "Epoca 1, Batch 600, Loss: 0.346\n",
      "Epoca 1, Batch 700, Loss: 0.320\n",
      "Epoca 1, Batch 800, Loss: 0.324\n",
      "Epoca 1, Batch 900, Loss: 0.317\n",
      "Epoca 2, Batch 100, Loss: 0.279\n",
      "Epoca 2, Batch 200, Loss: 0.275\n",
      "Epoca 2, Batch 300, Loss: 0.249\n",
      "Epoca 2, Batch 400, Loss: 0.261\n",
      "Epoca 2, Batch 500, Loss: 0.268\n",
      "Epoca 2, Batch 600, Loss: 0.245\n",
      "Epoca 2, Batch 700, Loss: 0.254\n",
      "Epoca 2, Batch 800, Loss: 0.259\n",
      "Epoca 2, Batch 900, Loss: 0.236\n",
      "Epoca 3, Batch 100, Loss: 0.208\n",
      "Epoca 3, Batch 200, Loss: 0.193\n",
      "Epoca 3, Batch 300, Loss: 0.224\n",
      "Epoca 3, Batch 400, Loss: 0.210\n",
      "Epoca 3, Batch 500, Loss: 0.206\n",
      "Epoca 3, Batch 600, Loss: 0.208\n",
      "Epoca 3, Batch 700, Loss: 0.199\n",
      "Epoca 3, Batch 800, Loss: 0.208\n",
      "Epoca 3, Batch 900, Loss: 0.201\n",
      "Epoca 4, Batch 100, Loss: 0.167\n",
      "Epoca 4, Batch 200, Loss: 0.169\n",
      "Epoca 4, Batch 300, Loss: 0.176\n",
      "Epoca 4, Batch 400, Loss: 0.175\n",
      "Epoca 4, Batch 500, Loss: 0.167\n",
      "Epoca 4, Batch 600, Loss: 0.173\n",
      "Epoca 4, Batch 700, Loss: 0.185\n",
      "Epoca 4, Batch 800, Loss: 0.168\n",
      "Epoca 4, Batch 900, Loss: 0.183\n",
      "Epoca 5, Batch 100, Loss: 0.144\n",
      "Epoca 5, Batch 200, Loss: 0.131\n",
      "Epoca 5, Batch 300, Loss: 0.142\n",
      "Epoca 5, Batch 400, Loss: 0.154\n",
      "Epoca 5, Batch 500, Loss: 0.142\n",
      "Epoca 5, Batch 600, Loss: 0.144\n",
      "Epoca 5, Batch 700, Loss: 0.144\n",
      "Epoca 5, Batch 800, Loss: 0.139\n",
      "Epoca 5, Batch 900, Loss: 0.139\n",
      "Epoca 6, Batch 100, Loss: 0.110\n",
      "Epoca 6, Batch 200, Loss: 0.119\n",
      "Epoca 6, Batch 300, Loss: 0.107\n",
      "Epoca 6, Batch 400, Loss: 0.108\n",
      "Epoca 6, Batch 500, Loss: 0.118\n",
      "Epoca 6, Batch 600, Loss: 0.116\n",
      "Epoca 6, Batch 700, Loss: 0.118\n",
      "Epoca 6, Batch 800, Loss: 0.125\n",
      "Epoca 6, Batch 900, Loss: 0.110\n",
      "Epoca 7, Batch 100, Loss: 0.091\n",
      "Epoca 7, Batch 200, Loss: 0.087\n",
      "Epoca 7, Batch 300, Loss: 0.087\n",
      "Epoca 7, Batch 400, Loss: 0.084\n",
      "Epoca 7, Batch 500, Loss: 0.098\n",
      "Epoca 7, Batch 600, Loss: 0.096\n",
      "Epoca 7, Batch 700, Loss: 0.102\n",
      "Epoca 7, Batch 800, Loss: 0.096\n",
      "Epoca 7, Batch 900, Loss: 0.098\n",
      "Epoca 8, Batch 100, Loss: 0.066\n",
      "Epoca 8, Batch 200, Loss: 0.066\n",
      "Epoca 8, Batch 300, Loss: 0.073\n",
      "Epoca 8, Batch 400, Loss: 0.074\n",
      "Epoca 8, Batch 500, Loss: 0.068\n",
      "Epoca 8, Batch 600, Loss: 0.077\n",
      "Epoca 8, Batch 700, Loss: 0.077\n",
      "Epoca 8, Batch 800, Loss: 0.073\n",
      "Epoca 8, Batch 900, Loss: 0.077\n",
      "Epoca 9, Batch 100, Loss: 0.047\n",
      "Epoca 9, Batch 200, Loss: 0.046\n",
      "Epoca 9, Batch 300, Loss: 0.049\n",
      "Epoca 9, Batch 400, Loss: 0.047\n",
      "Epoca 9, Batch 500, Loss: 0.069\n",
      "Epoca 9, Batch 600, Loss: 0.062\n",
      "Epoca 9, Batch 700, Loss: 0.061\n",
      "Epoca 9, Batch 800, Loss: 0.066\n",
      "Epoca 9, Batch 900, Loss: 0.056\n",
      "Epoca 10, Batch 100, Loss: 0.036\n",
      "Epoca 10, Batch 200, Loss: 0.030\n",
      "Epoca 10, Batch 300, Loss: 0.037\n",
      "Epoca 10, Batch 400, Loss: 0.036\n",
      "Epoca 10, Batch 500, Loss: 0.038\n",
      "Epoca 10, Batch 600, Loss: 0.058\n",
      "Epoca 10, Batch 700, Loss: 0.046\n",
      "Epoca 10, Batch 800, Loss: 0.054\n",
      "Epoca 10, Batch 900, Loss: 0.050\n",
      "Addestramento completato.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, trainloader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza sul set di test: 92.00%\n",
      "Accuratezza sul set di test: 98.95%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disattiva il calcolo dei gradienti per la valutazione\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuratezza sul set di test: {100 * correct / total:.2f}%')\n",
    "\n",
    "evaluate_model(model, testloader)\n",
    "evaluate_model(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top: 0.00%\n",
      "Trouser: 0.00%\n",
      "Pullover: 0.00%\n",
      "Dress: 0.00%\n",
      "Coat: 0.00%\n",
      "Sandal: 0.00%\n",
      "Shirt: 0.00%\n",
      "Sneaker: 0.00%\n",
      "Bag: 100.00%\n",
      "Ankle boot: 0.00%\n",
      "Predicted Class: Bag\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.466979741489906e-12,\n",
       " 9.775405311607255e-14,\n",
       " 2.9103324148718713e-16,\n",
       " 8.017393803442522e-19,\n",
       " 8.494779897474305e-19,\n",
       " 1.0208819929029377e-17,\n",
       " 4.252282213344327e-16,\n",
       " 3.8835007895658874e-19,\n",
       " 1.0,\n",
       " 9.827704005682756e-16]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Classi del Fashion MNIST\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Preprocessa l'immagine per Fashion MNIST\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Assicura che l'immagine sia in scala di grigi\n",
    "        transforms.Resize((28, 28)),  # Ridimensiona l'immagine a 28x28 pixel come Fashion MNIST\n",
    "        transforms.ToTensor(),  # Converti l'immagine in tensore\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalizza l'immagine per grayscale (1 canale)\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Aggiungi la dimensione batch\n",
    "    return img\n",
    "\n",
    "# Funzione per fare le previsioni e stampare le probabilità\n",
    "def predict_confidence(model, image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    \n",
    "    # Disabilita il calcolo dei gradienti per inferenza\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "    \n",
    "    # Applica softmax per ottenere le probabilità\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Converti il tensore in lista di probabilità per ciascuna classe\n",
    "    probabilities_list = probabilities.squeeze().tolist()\n",
    "    \n",
    "    # Stampa le probabilità per ciascuna classe\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f'{class_name}: {probabilities_list[i] * 100:.2f}%')\n",
    "\n",
    "    # Predici la classe con la probabilità più alta\n",
    "    predicted_class_idx = torch.argmax(probabilities).item()\n",
    "    predicted_class = classes[predicted_class_idx]\n",
    "    print(f'Predicted Class: {predicted_class}')\n",
    "\n",
    "    return probabilities_list\n",
    "\n",
    "# Esempio di utilizzo: prevedere su un'immagine esterna\n",
    "image_path = 'maglietta-azzurra.jpg'  # Modifica questo percorso con l'immagine che vuoi testare\n",
    "predict_confidence(model, image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
